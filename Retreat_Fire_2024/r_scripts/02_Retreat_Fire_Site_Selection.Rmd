---
title: "02_Retreat_Fire_Site_Selection"
output: html_document
date: "2024-08-19"
---

---
title: "geospatial_check_comids"
output: html_document
date: "2024-08-19"
---

# Status: in progress

# ==============================================================================
# Author: Kathryn Willi & Matthew R. V. Ross: https://doi.org/10.5281/zenodo.8140272

# Adapted by Jake Cavaiani for site selection for the Retreat Fire (2024)

# 19 August 2024
# ==============================================================================


```{r setup, include=TRUE, echo = T, warning = F, comment = F, message = FALSE}
rm(list = ls())
library(sf)
library(tidyverse)
library(terra)
library(nhdplusTools)
library(mapview)
library(dataRetrieval)
library(lubridate)
library(prism)
library(ggspatial)
library(nngeo)# Added from original code
library(stars)# Added from original code
library(here)
library(webshot)
here()
# this gives you an error, but it can be ignored:
try(plyr::ldply(list.files(path = here("geospatial_data_functions", "src"),
                           pattern="*.R",
                           full.names=TRUE),
                source))
# Rmarkdown options
knitr::opts_chunk$set(echo = T, warning = F, comment = F, message = F)

# mapview options
mapviewOptions(basemaps.color.shuffle=FALSE,basemaps='OpenTopoMap')

```

### Setting up your site data set.

For this code to run properly, your site data must be configured as follows:

1)  Each site is identified with a unique site name. In the data set, this column must be called `site`.
2)  Each site has coordinates, with column names `longitude` and `latitude`. Knowledge of coordinate projection required. **OR:**
    Each site has their known COMID, with column name `comid`. 
4)  Site data table is a CSV, and stored in the `data/` folder. 

I have included an example data set called `placeholder.csv`, along with all of the additional data sets necessary for the code to run, stored in the `data` folder. 

#### Downloading necessary data sets

Currently, this workflow requires downloading several data sets locally for much speedier run times. This includes: PRISM climate & aridity rasters, NHD flow direction data, and CONUS-wide NHD catchments. All data sets are found in the shared `data` folder.

### Site data assumptions.

This analysis is only appropriate for locations along adequately-sized streams. Some streams are too small to be captured by NHDPlusV2; it is also common for coordinates to fall in the wrong catchment (especially for big rivers). For that reason, review each site and make sure that the NHD feature attributes were appropriately captured.

# National Hydrodraphy Dataset (NHD) data extraction

Identify each sample's NHD COMID. This COMID will allow site linkages with all datasets in this workflow. If COMID is already listed in the CSV, make `site_type = "comid"`.

```{r}
sf_use_s2(FALSE)

site_type = "xy" # OR site_type = "comid"

sites <- read_csv(here("Retreat_Fire_2024", "site_selection", "Site_List_8_19_2024.csv"), na = c('-9999', 'N/A')) %>%
  distinct(site, .keep_all = TRUE) %>%
  dplyr::select('site','latitude','longitude') %>% 
  drop_na()

```

Additional steps for sites with coordinates, and no COMID:

```{r}
if(site_type == "xy"){
  sites <- sites %>%
    dplyr::select(site, latitude, longitude) %>% 
    sf::st_as_sf(coords = c("longitude","latitude"), crs = 4269) # 4269 = NAD83 CRS
  
  if(sf::st_crs(sites) != sf::st_crs(4269)){
    sites <- sites %>% st_transform(., crs = 4269)
  }
  
  mapview(sites)
}
```

Pull all meta data associated with each site's COMID. 

```{r}
# oak creek tribs #
sites <- sites %>% 
  filter(site %in% c('Clint Canyon', 'Pine Tree Canyon',
                     'Hoover Canyon', 'Indian Creek',
                     'North Fork Oak Creek', 'Counterfeit Creek',
                     'South Fork Oak Creek', 'unnamed_1',
                     'unnamed_2', 'unnamed_3', 'Oak_Creek_upstream')) 
                     

# Tieton Creek tribs #
sites <- sites %>% 
  filter(site %in% c('Cabin Creek', 'Sentinel Creek',
                     'South Tieton_unnamed_415', 'South Tieton_unnamed_1302', 
                     'Milk Creek'))

# North Fork Cowiche Creek COMID - 24423497
if(site_type == "xy"){
  sites <- getNHDxy(df = sites)
}


```

Make NHD-based watershed shapefiles for all CONUS sites. To make this step MUCH faster, it is best to have a locally downloaded version on the National NHD catchment shapefile stored on your local system. I have already included this shapefile in the `data` folder. 

```{r}
site_watersheds <- getWatersheds(df = sites, make_pretty = TRUE) %>%
  inner_join(., dplyr::select(sf::st_drop_geometry(sites), site, comid), by = "comid")

# Write the sf object to a shapefile for Oak Creek Tribs
st_write(site_watersheds, here("Retreat_Fire_2024", "output_for_analysis", "02_Retreat_Fire_Site_Selection", "Oak_Creek_tribs.shp"))

# Export geospatial data for Oak Creek Tribs
write_csv(sites, here("Retreat_Fire_2024", "output_for_analysis", "02_Retreat_Fire_Site_Selection", "Oak_Creek_tribs_geospatial.csv"))

# Write the sf object to a shapefile for Tieton Tribs
st_write(site_watersheds, here("Retreat_Fire_2024", "output_for_analysis", "02_Retreat_Fire_Site_Selection", "Tieton_tribs.shp"))

# Export geospatial data for Tieton Tribs
write_csv(sites, here("Retreat_Fire_2024", "output_for_analysis", "02_Retreat_Fire_Site_Selection", "Tieton_tribs_geospatial.csv"))


```

Ensure that each site is accurately represented by the NHD (some locations may be located on streams that are too small to be captured by the NHD, others may have coordinates that are slightly off, potentially placing them in the wrong catchment). Here, we create simple maps of each watershed, to determine if the delineated watershed/NHD attributes are appropriate. This can take a while depending on how many sites you are doing this for. Maps are automatically stored in the `data/maps/` folder. It is highly recommended to review each map, particularly for known locations along BIG rivers and TINY streams.

```{r}
map2(sites$site, sites$comid, getMaps) 
```

Interactive map showing all sites and their delineated watersheds:

```{r}
mapview(site_watersheds, col.regions = "#56B4E9", alpha.regions = 0.2, lwd = 3, layer.name = "Watershed") +
  mapview(sites, cex = 5, col.regions = "black", layer.name = "Points") + 
  mapview(st_read('~/GitHub/rc_sfa-fire-watch/geospatial_data_functions/src/site_flowlines.gpkg', quiet = T), lwd = 3, color = "red", layer.name = "Flowline")
```

```{r noncat,echo = T, warning = FALSE, comment = F, message = FALSE, results='hide'}
# Extract the mean aridity index within each site's watershed as well as each site's location
arid <- getAridity(df = sites, sf = site_watersheds) #.tif doesn't exist 
sites <- getAridity(df = sites, sf = site_watersheds)

# Extract Omernik ecoregion for each site's location
sites <- getOmernikSite(df = sites)

# Extract dominant Omernik ecoregion within each site's watershed
sites <- getOmernikWs(df = sites, sf = site_watersheds)

# Extract PRSIM ppt, tmean, tmax, and tmin data for each site's location
sites_prism <- getPRISM(df = sites)

# # Extract mean chemistry values within each site's watershed as well as each site's location
# sites <- getChemistry(df = sites, sf = site_watersheds)

# Link to original NPP data set:
# https://lpdaac.usgs.gov/products/mod17a3hgfv006/ "Terra MODIS Net Primary Production Yearly L4 Global 500 m SIN Grid products are currently unavailable due to unexpected errors in the input data. Please note that a newer version of MODIS land products is available and plans are being developed for the retirement of Version 6 MODIS data products. Users are advised to transition to the improved Version 6.1 products as soon as possible."
```

```{r filter watershed characteristics to Schneider Springs site selection data frame}

output_file <- sites %>% 
  dplyr::select(site, comid, geometry, streamorde, slope, totdasqkm, maxelevraw)

```


### ONLY IF NEEDED View sites and adjacent COMIDs ####

If after reviewing the delineated watersheds you have found that some site coordinates place that location in the wrong catchment, we will need to explore that site's nearby catchments to link it to the correct COMID:

```{r}
sus_points <- sites
  # Comment the line below if you want to remove some sites:
  # %>%
  #filter(site %in% c("site_name","sine_name2"))

sus_nhd <- sus_points %>%
  sf::st_buffer(., dist = 0.01) %>%
  sus_mapper(.)


mapview(sus_nhd[[2]], col.regions = "#56B4E9", alpha.regions = 0.4, legend = FALSE) +
 # mapview(sbux_sf, cex = 5, col.regions = "red", legend = FALSE) +
  mapview(sus_points, cex = 5, col.regions = "black", legend = FALSE) +
  mapview(sus_nhd[[1]], lwd = 1.5, color = "red", legend = FALSE)

# Adjacent COMID: 24423499

```

## In the table below, add sites and their correct comid's

```{r}
# # # Based on the map, it appears that this site should be linked to comid a different comid
# 
# updated_sites <- tibble(site = c("Site_3", "Reference", "Payson", "Dry_Creek"),comid = c(8943613, 15032969, 10350360, 10329181)) 
# # Site 3 in the Oliver paper is supposed to be this COMID: 8943613 
# 
# # Reference from the Uzun paper is supposed to be COMID 15032969
# 
# # Payson from Crandall paper is supposed to be COMID: 10350360
# 
# # Dry Creek in Crandall paper is supposed to be COMID: 10329181
# 
# sites <- updated_sites %>% 
#   getNHDcomid(.) %>% 
#   bind_rows(filter(sites, site %in% sus_points$site)) %>% 
#   distinct(site, .keep_all = TRUE)
# 
# sites <- sites %>% 
#   dplyr::select(site, comid, totdasqkm, minelevraw, maxelevsmo, minelevsmo, elevfixed, slope) %>% 
#   rename(Site = site)
# 
# site_watersheds <- getWatersheds(df = sites, make_pretty = TRUE) %>%
#   inner_join(., dplyr::select(sf::st_drop_geometry(sites), site, comid), by = "comid")
# 
# write_csv(sites, here("Output_for_analysis", "05_Meta_geospatial_check_comids", "geospatial_data.csv"))
# 
# site_watersheds <- site_watersheds %>% 
#   rename(Site = site)
# write_csv(site_watersheds, here("Output_for_analysis", "05_Meta_geospatial_check_comids", "meta_comids.csv"))


```


```{r export COMIDs}
# site_type = "comid"
# 
# if(site_type == "comid"){
#   sites <- getNHDcomid(df = dplyr::select(sites, site, comid))
# }
# 
# 
# sites_export <- sites %>%
#   select(site, latitude, longitude, comid)
# 
# write_csv(sites_export, here("inputs", "catchment_characteristics", "meta_lat_long_comids.csv"))



```


### Investigate Oak Creek precip data within HUC10s with data 
```{r load in Huc10 shape file from Micah}
# Plot HUCs with data
huc10_with_data <- st_read(here("inputs", "Huc10_withData", "Huc10_withData.shp")) 

# Filter the proper Huc that includes Oak Creek within the Retreat Fire perimeter
little_naches_huc <- huc10_with_data %>% 
  filter(name == "Little Naches River")

# Plotting to make sure it is correct
ggplot(data = little_naches_huc) +
  geom_sf(fill = "darkgreen", alpha = 0.2, color = "black") + 
  theme_bw()

# Plot 30-year precip data #
little_naches_precip_long <- little_naches_huc %>% 
  select(starts_with("precip")) %>% 
  pivot_longer(
    cols = precip_jan:precip_dec,
    names_to = "Month",
    values_to = "Precip_mm"
  )

# Reorder factor levels using fct_relevel()
little_naches_precip_long$Month <- fct_relevel(little_naches_precip_long$Month, "precip_jan", "precip_feb", "precip_mar", "precip_apr", "precip_may", "precip_jun", "precip_Jul", "precip_aug", "precip_sep", "precip_oct", "precip_nov", "precip_dec")

ggplot(little_naches_precip_long, aes(x = Month, y = Precip_mm)) +
  geom_col() +
  scale_x_discrete(labels = c("precip_jan" = "Jan", 
                              "precip_feb" = "Feb",
                              "precip_mar" = "Mar", 
                              "precip_apr" = "Apr",
                              "precip_may" = "May", 
                              "precip_jun" = "June", 
                              "precip_Jul" = "July", 
                              "precip_aug" = "Aug",
                              "precip_sep" = "Sep", 
                              "precip_oct" = "Oct",
                              "precip_nov" = "Nov",
                              "precip_dec" = "Dec")) +
  ggtitle("Monthly Precip for Little Naches Huc10") +
  theme_bw()

ggsave("little_naches_huc_precip.pdf",
       path = here("plots", "climate"),
       width = 8, height = 6,  units = "in")


```

# Hydromet TICW (could use as an Oak Creek Proxy precip data) 
```{r}
ticw_daily_precip <- read_csv(here("inputs", "Climate", "tieton_ticw_daily_precip.csv"), 
                              skip = 1)

ticw_daily_precip <- ticw_daily_precip %>% 
  mutate(DateTime = mdy(DateTime)) %>% 
  na.omit(ticw_pp)

# Filter the dates with precipitation greater than 2 inches
high_precip_days <- ticw_daily_precip %>%
  filter(ticw_pp > 2) %>%
  pull(DateTime)


ggplot(ticw_daily_precip, aes(x = DateTime, y = ticw_pp)) +
  geom_bar(stat = "identity") + 
  scale_y_reverse() +
  ylim(5,0) +
  geom_vline(xintercept = as.numeric(high_precip_days), color = "red", linetype = "dashed") +  
  labs(x = "Date", y = "Precipitation (In)", title = "Daily Precipitation Time Series") +
  theme_bw()  

```







